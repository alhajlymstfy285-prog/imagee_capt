# Transformer Model Configuration

model:
  model_type: transformer
  embed_dim: 512
  num_heads: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  feedforward_dim: 2048
  dropout: 0.1
  
# GloVe Embeddings (optional)
embeddings:
  use_glove: false  # Transformers typically use learned embeddings
  glove_path: "glove.6B.300d.txt"
  freeze: false

# Training parameters
training:
  num_epochs: 100
  batch_size: 32
  learning_rate: 0.0001
  weight_decay: 0.0001
  grad_clip: 1.0
  warmup_steps: 4000
  
# Data
data:
  max_caption_length: 20
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

# Regularization
regularization:
  dropout: 0.1
  label_smoothing: 0.1

# Optimization
optimizer:
  type: adam
  betas: [0.9, 0.98]
  eps: 1.0e-9

# Scheduler
scheduler:
  type: warmup_cosine
  warmup_steps: 4000
  max_steps: 100000

# Checkpointing
checkpoint:
  save_every: 10
  save_best: true
  metric: val_loss
