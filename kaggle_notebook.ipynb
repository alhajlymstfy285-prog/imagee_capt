{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning - Training on Kaggle\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Add **Flickr Image Dataset** from Kaggle datasets\n",
    "2. Enable **GPU** in Settings → Accelerator\n",
    "3. Enable **Internet** in Settings\n",
    "4. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your GitHub repository\n",
    "!git clone https://github.com/<YOUR-USERNAME>/<YOUR-REPO>.git\n",
    "%cd <YOUR-REPO>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Flickr dataset\n",
    "!python kaggle_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python training/Vanilla_RNN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results\n",
    "with open('results/Vanilla_RNN/results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Vanilla RNN Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Parameters: {results['num_params']:,}\")\n",
    "print(f\"Final Train Loss: {results['final_train_loss']:.4f}\")\n",
    "print(f\"Final Val Loss: {results['final_val_loss']:.4f}\")\n",
    "print(f\"Best Val Loss: {results['best_val_loss']:.4f}\")\n",
    "print(f\"Total Time: {results['total_time']/60:.1f} minutes\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results['train_loss_history'], label='Train', linewidth=2)\n",
    "plt.plot(results['val_loss_history'], label='Val', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training History', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(results['val_loss_history'], linewidth=2, color='orange')\n",
    "plt.axhline(y=results['best_val_loss'], color='r', linestyle='--', label=f\"Best: {results['best_val_loss']:.2f}\")\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Loss', fontsize=12)\n",
    "plt.title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "gap = [v - t for v, t in zip(results['val_loss_history'], results['train_loss_history'])]\n",
    "plt.plot(gap, linewidth=2, color='red')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Val - Train Loss', fontsize=12)\n",
    "plt.title('Overfitting Gap', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Other Models (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "!python training/LSTM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Attention LSTM\n",
    "!python training/Attention_LSTM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer\n",
    "!python training/Transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "models = ['Vanilla_RNN', 'LSTM', 'Attention_LSTM', 'Transformer']\n",
    "results_list = []\n",
    "\n",
    "for model in models:\n",
    "    try:\n",
    "        with open(f'results/{model}/results.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            results_list.append({\n",
    "                'Model': model,\n",
    "                'Parameters': f\"{data['num_params']:,}\",\n",
    "                'Train Loss': f\"{data['final_train_loss']:.4f}\",\n",
    "                'Val Loss': f\"{data['final_val_loss']:.4f}\",\n",
    "                'Best Val': f\"{data['best_val_loss']:.4f}\",\n",
    "                'Time (min)': f\"{data['total_time']/60:.1f}\"\n",
    "            })\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ {model} not trained yet\")\n",
    "\n",
    "if results_list:\n",
    "    df = pd.DataFrame(results_list)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.Vanilla_RNN import VanillaRNNCaptioner\n",
    "from a5_helper import load_coco_captions, decode_captions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data and model\n",
    "data = load_coco_captions(\"./datasets/flickr.pt\")\n",
    "word_to_idx = data[\"vocab\"][\"token_to_idx\"]\n",
    "idx_to_word = data[\"vocab\"][\"idx_to_token\"]\n",
    "\n",
    "model = VanillaRNNCaptioner(\n",
    "    word_to_idx=word_to_idx,\n",
    "    wordvec_dim=128,\n",
    "    hidden_dim=128,\n",
    "    ignore_index=word_to_idx.get(\"<NULL>\")\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "model.load_state_dict(torch.load('results/Vanilla_RNN/model.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Generate captions for sample images\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_samples = 5\n",
    "sample_images = data[\"val_images\"][:num_samples].to(device)\n",
    "sample_captions_gt = data[\"val_captions\"][:num_samples]\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_captions = model.sample(sample_images)\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "for i in range(num_samples):\n",
    "    img = sample_images[i].cpu().permute(1, 2, 0)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    gt_caption = decode_captions(sample_captions_gt[i], idx_to_word)\n",
    "    gen_caption = decode_captions(generated_captions[i], idx_to_word)\n",
    "    \n",
    "    axes[i].set_title(f\"GT: {gt_caption}\\nGen: {gen_caption}\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
